if (!dir.exists(out_pre_nc)) {
dir.create(out_pre_nc, recursive = TRUE)
}
# Save the downloaded file to the destination folder
httr::write_disk(response, path = file_path, overwrite = TRUE)
cat(file_name, 'downloaded successfully.\n')
}, error = function(err) {
# Handle HTTP errors here if needed
cat('Error:', conditionMessage(err), '\n')
})
}
}
# Example usage
out_pre_nc <- file.path("~/Desktop/UF/R", "out_pre_nc")
get_prelim_nc(as.Date("2022-01-01"), as.Date("2022-12-31"), out_pre_nc)
get_prelim_nc <- function(dt_s, dt_e, out_pre_nc) {
# Create a session for HTTP requests with custom parameters
s <- httr::handle("https://data.chc.ucsb.edu")
# Iterate over each year between the start year and end year, inclusive
for (y in seq(as.numeric(format(dt_s, "%Y")), as.numeric(format(dt_e, "%Y")), by = 1)) {
single_y <- as.character(y)  # Convert the current year to a string
file_name <- paste0('prelim_nc_', single_y, '.nc')  # File name to download
file_path <- file.path(out_pre_nc, file_name)  # Full file path
tryCatch({
# Download the file from the specified URL
response <- httr::GET(paste0('https://data.chc.ucsb.edu/products/CHIRPS-2.0/prelim/global_daily/fixed/netcdf/chirps-v2.0.',
single_y, '.days_p05.nc',sep=""), timeout(680))
httr::stop_for_status(response)  # Check if the request was successful
# Create the destination folder if it doesn't already exist
if (!dir.exists(out_pre_nc)) {
dir.create(out_pre_nc, recursive = TRUE)
}
# Save the downloaded file to the destination folder
httr::write_disk(response, path = file_path, overwrite = TRUE)
cat(file_name, 'downloaded successfully.\n')
}, error = function(err) {
# Handle HTTP errors here if needed
cat('Error:', conditionMessage(err), '\n')
})
}
}
# Example usage
out_pre_nc <- file.path("~/Desktop/UF/R", "out_pre_nc")
get_prelim_nc(as.Date("2022-01-01"), as.Date("2022-12-31"), out_pre_nc)
get_prelim_nc(as.Date("2022-01-01"), as.Date("2022-01-31"), out_pre_nc)
for (k in 2020:2022) {
download.file(paste("https://iridl.ldeo.columbia.edu/SOURCES/.UCSB/.CHIRPS/.v2p0/.daily-improved/.global/.0p05/.prcp/Y/-40/0.5/40/GRID/X/-25/0.5/55/GRID/T/(1%20Jan%20",k,")/(31%20ODecc%20",k,")/RANGEEDGES/data.nc",sep=""),destfile =paste("Data/",k,".nc",sep = "" ))
}
for (k in 2020:2022) {
download.file(paste("https://iridl.ldeo.columbia.edu/SOURCES/.UCSB/.CHIRPS/.v2p0/.daily-improved/.global/.0p05/.prcp/Y/-40/0.5/40/GRID/X/-25/0.5/55/GRID/T/(1%20Jun%20",k,")/(31%20Octc%20",k,")/RANGEEDGES/data.nc",sep=""),destfile =paste("Data/",k,".nc",sep = "" ))
}
for (k in 2020:2022) {
download.file(paste("https://iridl.ldeo.columbia.edu/SOURCES/.UCSB/.CHIRPS/.v2p0/.daily-improved/.global/.0p05/.prcp/Y/-40/0.5/40/GRID/X/-25/0.5/55/GRID/T/(1%20Jun%20",k,")/(31%20Octc%20",k,")/RANGEEDGES/data.nc",sep=""),destfile =paste("Data/",k,".nc",sep = "" ))
}
library(dplyr)
library(tidync)
library(ncdf4)
library(rio)
library(rgdal)
library(ggplot2)
library(metR)
library(raster)
for (k in 2020:2022) {
download.file(paste("https://iridl.ldeo.columbia.edu/SOURCES/.UCSB/.CHIRPS/.v2p0/.daily-improved/.global/.0p05/.prcp/Y/-40/0.5/40/GRID/X/-25/0.5/55/GRID/T/(1%20Jun%20",k,")/(31%20Octc%20",k,")/RANGEEDGES/data.nc",sep=""),destfile =paste("Data/",k,".nc",sep = "" ))
}
paste("Data/",k,".nc",sep = "" )
dir.create("Data",recursive = T,showWarnings = F)
for (k in 2020:2022) {
download.file(paste("https://iridl.ldeo.columbia.edu/SOURCES/.UCSB/.CHIRPS/.v2p0/.daily-improved/.global/.0p05/.prcp/Y/-40/0.5/40/GRID/X/-25/0.5/55/GRID/T/(1%20Jun%20",k,")/(31%20Octc%20",k,")/RANGEEDGES/data.nc",sep=""),destfile =paste("Data/",k,".nc",sep = "" ))
}
for (k in 2020:2022) {
download.file(paste("https://iridl.ldeo.columbia.edu/SOURCES/.UCSB/.CHIRPS/.v2p0/.daily-improved/.global/.0p05/.prcp/Y/-40/0.5/40/GRID/X/-25/0.5/55/GRID/T/(1%20Jan%20",k,")/(31%20Decc%20",k,")/RANGEEDGES/data.nc",sep=""),destfile =paste("Data/",k,".nc",sep = "" ))
}
# Example usage
out_pre_nc <- file.path("~/Desktop/UF/R", "out_pre_nc")
get_prelim_nc(as.Date("2022-01-01"), as.Date("2022-01-31"), out_pre_nc)
library(raster)
library(lubridate)
chirps1 <- function(in_file, in_nc_dir, outprec_file) {
nc_lst <- list.files(in_nc_dir, pattern = "\\.nc$", full.names = TRUE)  # Retrieve the list of all .nc files in the input folder.
nc_lst <- sort(nc_lst)  # Sort the files in chronological order.
df_chirps <- data.frame()  # Create an empty data frame to store precipitation data.
in_pt <- readLines(in_file)  # Read lines from the input file.
in_pt <- in_pt[-1]  # Exclude the header line
for (row in in_pt) {
cols <- strsplit(row, ",")[[1]]  # Split the row by comma
id <- as.integer(cols[1])  # Get the ID from the first column
lat <- as.numeric(cols[2])  # Get the latitude from the second column
lon <- as.numeric(cols[3])  # Get the longitude from the third column
time_lst <- vector("character")  # Vector to store time values
precval <- vector("numeric")  # Vector to store precipitation values
for (nc_file in nc_lst) {
if (grepl("\\.nc$", nc_file)) {  # Check if the file is in NetCDF format
dsi <- raster::brick(nc_file)  # Read the NetCDF file as a raster brick
meta_nc <- attr(dsi, "ncdfattr")  # Get the metadata of the NetCDF file
date_start <- substr(meta_nc$`time#units`, start = 12, stop = 30)  # Get the start date from the metadata
datetime_st <- as.POSIXlt(date_start, format = "%Y-%m-%d %H:%M:%S")  # Convert the start date to a POSIXlt object
bands_time <- meta_nc$NETCDF_DIM_time_VALUES[2:length(meta_nc$NETCDF_DIM_time_VALUES) - 1]  # Get the time dimension values
bands_time <- as.integer(bands_time)  # Convert the values to integers
gt <- geotrans(dsi)  # Get the geotransformation of the NetCDF file
px <- floor((lon - gt[1]) / gt[2]) + 1  # Calculate the X coordinate of the pixel corresponding to the longitude
py <- floor((lat - gt[4]) / gt[5]) + 1  # Calculate the Y coordinate of the pixel corresponding to the latitude
bands <- nlayers(dsi)  # Get the number of bands in the NetCDF file
for (i in 1:bands) {
d <- extract(dsi[[i]], cbind(px, py))  # Extract the precipitation value for the given pixel
dt_st <- datetime_st + ddays(bands_time[i])  # Calculate the date corresponding to the band
band_t <- format(dt_st, format = "%Y%j")  # Convert the date to YJJJ format (year and day of the year)
time_lst <- c(time_lst, band_t)  # Add the time value to the vector
if (is.na(d)) {
d <- -9999.0  # Replace missing values with a specific code
}
precval <- c(precval, d)  # Add the precipitation value to the vector
}
}
}
df_chirps[id] <- precval  # Associate the vector of precipitation values with the ID in the data frame
}
df_chirps <- as.data.frame(t(df_chirps))  # Transpose the data frame to have dates as columns
colnames(df_chirps) <- time_lst  # Associate the time values with column names
rownames(df_chirps) <- "ID"  # Assign a name to the row index
out_folder <- dirname(outprec_file)
if (!dir.exists(out_folder)) {
dir.create(out_folder, recursive = TRUE)  # Create the destination directory of the output file if it doesn't exist
}
saveRDS(df_chirps, file = outprec_file)  # Save the data frame to a file in RDS format
}
chirps1(in_file, in_nc_dir, outprec_file)
in_file= "~/Desktop/UF/R/Input1"
chirps1(in_file, in_nc_dir, outprec_file)
out_cor_nc="~/Desktop/UF/R/out_cor_nc"
in_file= "~/Desktop/UF/R/Input1"
outprec_file="~/Desktop/UF/R/vv"
chirps1(in_file, out_cor_nc, outprec_file)
in_file= "~/Desktop/UF/R/Input1.csv"
outprec_file="~/Desktop/UF/R/vv"
chirps1(in_file, out_cor_nc, outprec_file)
in_file= "~/Desktop/UF/R/Input1.csv"
outprec_file="~/Desktop/UF/R/vv"
chirps1(in_file, out_cor_nc, outprec_file)
chirps1 <- function(in_file, in_nc_dir, outprec_file) {
nc_lst <- list.files(in_nc_dir, pattern = "\\.nc$", full.names = TRUE)  # Retrieve the list of all .nc files in the input folder
nc_lst <- sort(nc_lst)  # Sort the files in chronological order
df_chirps <- data.frame()  # Create an empty data frame to store precipitation data
in_pt <- readLines(in_file)  # Read lines from the input file
for (row in in_pt) {
if (nchar(row) > 0) {  # Check if the row is not empty
cols <- strsplit(row, ",")[[1]]  # Split the row by comma
id <- as.integer(cols[1])  # Get the ID from the first column
lat <- as.numeric(cols[2])  # Get the latitude from the second column
lon <- as.numeric(cols[3])  # Get the longitude from the third column
time_lst <- vector("character")  # Vector to store time values
precval <- vector("numeric")  # Vector to store precipitation values
for (nc_file in nc_lst) {
if (grepl("\\.nc$", nc_file)) {  # Check if the file is in NetCDF format
dsi <- brick(nc_file)  # Read the NetCDF file as a raster brick
meta_nc <- attr(dsi, "ncdfattr")  # Get the metadata of the NetCDF file
date_start <- substr(meta_nc$`time#units`, start = 12, stop = 30)  # Get the start date from the metadata
datetime_st <- as.POSIXlt(date_start, format = "%Y-%m-%d %H:%M:%S")  # Convert the start date to a POSIXlt object
bands_time <- meta_nc$NETCDF_DIM_time_VALUES[2:length(meta_nc$NETCDF_DIM_time_VALUES) - 1]  # Get the time dimension values
bands_time <- as.integer(bands_time)  # Convert the values to integers
gt <- geotransform(dsi)  # Get the geotransformation of the NetCDF file
px <- floor((lon - gt[1]) / gt[2]) + 1  # Calculate the X coordinate of the pixel corresponding to the longitude
py <- floor((lat - gt[4]) / gt[6]) + 1  # Calculate the Y coordinate of the pixel corresponding to the latitude
bands <- nlayers(dsi)  # Get the number of bands in the NetCDF file
for (i in 1:bands) {
d <- extract(dsi[[i]], cbind(px, py))  # Extract the precipitation value for the given pixel
dt_st <- datetime_st + ddays(bands_time[i])  # Calculate the date corresponding to the band
band_t <- format(dt_st, format = "%Y%j")  # Convert the date to YJJJ format (year and day of the year)
time_lst <- c(time_lst, band_t)  # Add the time value to the vector
if (is.na(d)) {
d <- -9999.0  # Replace missing values with a specific code
}
precval <- c(precval, d)  # Add the precipitation value to the vector
}
}
}
df_chirps[id] <- precval  # Associate the vector of precipitation values with the ID in the data frame
}
}
df_chirps <- as.data.frame(t(df_chirps))  # Transpose the data frame to have dates as columns
colnames(df_chirps) <- time_lst  # Associate the time values with column names
rownames(df_chirps) <- "ID"  # Assign a name to the row index
out_folder <- dirname(outprec_file)
if (!dir.exists(out_folder)) {
dir.create(out_folder, recursive = TRUE)  # Create the destination directory of the output file if it doesn't exist
}
saveRDS(df_chirps, file = outprec_file)  # Save the data frame to a file in RDS format
}
in_file= "~/Desktop/UF/R/Input1.csv"
outprec_file="~/Desktop/UF/R/vv"
chirps1(in_file, out_cor_nc, outprec_file)
# Install and load the required packages
install.packages("flowmapr")
# Install and load the required packages
install.packages("ggraph")
# Install and load the required packages
install.packages("ggraph")
# Install and load the required packages
install.packages("ggraph")
# Installer et charger les bibliothèques nécessaires
install.packages("leaflet")
library(leaflet)
# Créer un jeu de données de flux migratoires (exemple fictif)
migrations <- data.frame(
pays_origine = c("Pays A", "Pays B", "Pays C"),
pays_destination = c("Pays D", "Pays E", "Pays D"),
flux = c(500, 700, 400)
)
View(migrations)
# Charger les coordonnées géographiques des pays (exemple fictif)
pays_coords <- data.frame(
pays = c("Pays A", "Pays B", "Pays C", "Pays D", "Pays E"),
latitude = c(10, 20, 30, 40, 50),
longitude = c(60, 70, 80, 90, 100)
)
# Fusionner les données de migration et de coordonnées
migrations_coords <- merge(migrations, pays_coords, by.x = "pays_origine", by.y = "pays", all.x = TRUE)
migrations_coords <- merge(migrations_coords, pays_coords, by.x = "pays_destination", by.y = "pays", all.x = TRUE, suffixes = c("_origine", "_destination"))
View(migrations_coords)
# Créer une carte leaflet
map <- leaflet() %>%
addProviderTiles("OpenStreetMap.Mapnik") # Vous pouvez choisir un autre fournisseur de tuiles si vous le souhaitez
# Ajouter les lignes de flux migratoires à la carte
for (i in 1:nrow(migrations_coords)) {
map <- map %>%
addPolylines(
lng = c(migrations_coords[i, "longitude_origine"], migrations_coords[i, "longitude_destination"]),
lat = c(migrations_coords[i, "latitude_origine"], migrations_coords[i, "latitude_destination"]),
weight = migrations_coords[i, "flux"] / 100,  # Ajustez l'épaisseur de ligne en fonction du flux migratoire
color = "red"  # Couleur de la ligne
)
}
# Afficher la carte
map
# Charger les coordonnées géographiques des pays (exemple fictif)
pays_coords <- data.frame(
pays = c("Pays A", "Pays B", "Pays C", "Pays D", "Pays E"),
latitude = c(-10, 20, 30, 40, 50),
longitude = c(60, 70, 80, 90, 100)
)
# Fusionner les données de migration et de coordonnées
migrations_coords <- merge(migrations, pays_coords, by.x = "pays_origine", by.y = "pays", all.x = TRUE)
migrations_coords <- merge(migrations_coords, pays_coords, by.x = "pays_destination", by.y = "pays", all.x = TRUE, suffixes = c("_origine", "_destination"))
# Créer une carte leaflet
map <- leaflet() %>%
addProviderTiles("OpenStreetMap.Mapnik") # Vous pouvez choisir un autre fournisseur de tuiles si vous le souhaitez
# Ajouter les lignes de flux migratoires à la carte
for (i in 1:nrow(migrations_coords)) {
map <- map %>%
addPolylines(
lng = c(migrations_coords[i, "longitude_origine"], migrations_coords[i, "longitude_destination"]),
lat = c(migrations_coords[i, "latitude_origine"], migrations_coords[i, "latitude_destination"]),
weight = migrations_coords[i, "flux"] / 100,  # Ajustez l'épaisseur de ligne en fonction du flux migratoire
color = "red"  # Couleur de la ligne
)
}
# Afficher la carte
map
library(leaflet)
library(sp)
# Créer un jeu de données de flux migratoires (exemple fictif)
migrations <- data.frame(
pays_origine = c("Pays A", "Pays B", "Pays C"),
pays_destination = c("Pays D", "Pays E", "Pays D"),
flux = c(500, 700, 400)
)
# Charger les coordonnées géographiques des pays (exemple fictif)
pays_coords <- data.frame(
pays = c("Pays A", "Pays B", "Pays C", "Pays D", "Pays E"),
latitude = c(10, 20, 30, 40, 50),
longitude = c(60, 70, 80, 90, 100)
)
# Charger les données des frontières des pays (exemple fictif)
pays_frontieres <- data.frame(
pays = c("Pays A", "Pays B", "Pays C", "Pays D", "Pays E"),
frontiere = c(1, 2, 3, 4, 5)
)
# Fusionner les données de migration, de coordonnées et de frontières
migrations_coords <- merge(migrations, pays_coords, by.x = "pays_origine", by.y = "pays", all.x = TRUE)
migrations_coords <- merge(migrations_coords, pays_coords, by.x = "pays_destination", by.y = "pays", all.x = TRUE, suffixes = c("_origine", "_destination"))
migrations_coords <- merge(migrations_coords, pays_frontieres, by = "pays")
# Créer une carte leaflet
map <- leaflet() %>%
addProviderTiles("OpenStreetMap.Mapnik") # Vous pouvez choisir un autre fournisseur de tuiles si vous le souhaitez
library(leaflet)
library(sp)
# Créer un jeu de données de flux migratoires (exemple fictif)
migrations <- data.frame(
pays_origine = c("Pays A", "Pays B", "Pays C"),
pays_destination = c("Pays D", "Pays E", "Pays D"),
flux = c(500, 700, 400)
)
# Charger les coordonnées géographiques des pays (exemple fictif)
pays_coords <- data.frame(
pays = c("Pays A", "Pays B", "Pays C", "Pays D", "Pays E"),
latitude = c(10, 20, 30, 40, 50),
longitude = c(60, 70, 80, 90, 100)
)
# Charger les données des frontières des pays (exemple fictif)
pays_frontieres <- data.frame(
pays = c("Pays A", "Pays B", "Pays C", "Pays D", "Pays E"),
frontiere = c(1, 2, 3, 4, 5)
)
# Fusionner les données de migration, de coordonnées et de frontières
migrations_coords <- merge(migrations, pays_coords, by.x = "pays_origine", by.y = "pays", all.x = TRUE)
migrations_coords <- merge(migrations_coords, pays_coords, by.x = "pays_destination", by.y = "pays", all.x = TRUE, suffixes = c("_origine", "_destination"))
migrations_coords <- merge(migrations_coords, pays_frontieres, by = "pays")
# Créer une carte leaflet
map <- leaflet() %>%
addProviderTiles("OpenStreetMap.Mapnik") # Vous pouvez choisir un autre fournisseur de tuiles si vous le souhaitez
# Ajouter les lignes de flux migratoires à la carte
for (i in 1:nrow(migrations_coords)) {
map <- map %>%
addPolylines(
lng = c(migrations_coords[i, "longitude_origine"], migrations_coords[i, "longitude_destination"]),
lat = c(migrations_coords[i, "latitude_origine"], migrations_coords[i, "latitude_destination"]),
weight = migrations_coords[i, "flux"] / 100,  # Ajustez l'épaisseur de ligne en fonction du flux migratoire
color = "red"  # Couleur de la ligne
)
}
# Obtenir les frontières des pays concernés
pays_concernes <- unique(c(migrations_coords$pays_origine, migrations_coords$pays_destination))
frontieres_concernees <- pays_frontieres[pays_frontieres$pays %in% pays_concernes, ]
# Créer un objet SpatialPolygonsDataFrame pour représenter les frontières des pays concernés
frontieres_sp <- borders(language = "fr", dataset = "countries", returnclass = "sp")
frontieres_concernees_sp <- frontier(frontieres_sp, ID = frontieres_concernees$frontiere)
# Ajouter les frontières des pays concernés à la carte
map <- map %>%
addPolygons(
data = frontieres_concernees_sp,
fillColor = "yellow",  # Couleur de remplissage des pays
fillOpacity = 0.4,  # Opacité de remplissage
color = "black",  # Couleur des frontières
weight = 1  # Épaisseur des frontières
)
# Installer et charger les bibliothèques nécessaires
install.packages("leaflet")
library(leaflet)
# Créer un jeu de données de flux migratoires (exemple fictif)
migrations <- data.frame(
pays_origine = c("Pays A", "Pays B", "Pays C"),
pays_destination = c("Pays D", "Pays E", "Pays D"),
flux = c(500, 700, 400)
)
# Charger les coordonnées géographiques des pays (exemple fictif)
pays_coords <- data.frame(
pays = c("Pays A", "Pays B", "Pays C", "Pays D", "Pays E"),
latitude = c(-10, 20, 30, 40, 50),
longitude = c(60, 70, 80, 90, 100)
)
# Fusionner les données de migration et de coordonnées
migrations_coords <- merge(migrations, pays_coords, by.x = "pays_origine", by.y = "pays", all.x = TRUE)
migrations_coords <- merge(migrations_coords, pays_coords, by.x = "pays_destination", by.y = "pays", all.x = TRUE, suffixes = c("_origine", "_destination"))
# Créer une carte leaflet
map <- leaflet() %>%
addProviderTiles("OpenStreetMap.Mapnik") # Vous pouvez choisir un autre fournisseur de tuiles si vous le souhaitez
# Créer une carte leaflet
map <- leaflet() %>%
addProviderTiles("OpenStreetMap.Mapnik") # Vous pouvez choisir un autre fournisseur de tuiles si vous le souhaitez
# Ajouter les lignes de flux migratoires à la carte
for (i in 1:nrow(migrations_coords)) {
map <- map %>%
addPolylines(
lng = c(migrations_coords[i, "longitude_origine"], migrations_coords[i, "longitude_destination"]),
lat = c(migrations_coords[i, "latitude_origine"], migrations_coords[i, "latitude_destination"]),
weight = migrations_coords[i, "flux"] / 100,  # Ajustez l'épaisseur de ligne en fonction du flux migratoire
color = "red"  # Couleur de la ligne
)
}
# Afficher la carte
map
migrations_coords
install.packages("spdep")
install.packages("tmap")
install.packages("tmap")
install.packages("lwgeom")
install.packages("lwgeom")
install.packages("motif")
# Download global CHIRPS
# By: H. Achicanoy
# December, 2022
# R options
g <- gc(reset = T); rm(list = ls()) # Empty garbage collector
options(warn = -1, scipen = 999)    # Remove warning alerts and scientific notation
suppressMessages(library(pacman))
suppressMessages(pacman::p_load(tidyverse,terra,lubridate,R.utils))
# Time frame
start <- 1983
end <- 2016
# Output directory
setwd("~/Desktop/BioD/")
Out <- "/Users/yacoub/Desktop/BioD/Migrations_Works/ClimateRiskPr/1.Data/Chirts/"
dir.create(Out, recursive = TRUE, showWarnings = FALSE)
#download CHIRTS data
#JRV, 2022
#load needed libraries
library(geodata)
library(tidyverse)
#function to get CHIRTS data (Tmax, Tmin, RH).
CHIRTS_data <- function(vars=c("Rh", "Tmax", "Tmin"), years=start:end, basedir=Out) {
#create table of files and file names e.g., RH.1983.01.01.tif
file_tb <- expand.grid(day=1:31, month=1:12, year=years, variable=vars)
#file_tb <- expand.grid(day=1:10, month=1, year=1983, variable=c("RH"))
filenames <- file_tb %>%
base::split(.[,c("day","month","year","variable")]) %>%
purrr::map(.f=function(.x) {
fname <- paste0(.x$variable, ".", .x$year, ".", sprintf("%02.0f", .x$month), ".", sprintf("%02.0f", .x$day), ".tif")
return(fname)
}) %>%
as.character(unlist(.))
file_tb <- file_tb %>%
dplyr::mutate(filename=filenames)
#base url
base_url <- "http://data.chc.ucsb.edu/products/CHIRTSdaily/v1.0/global_tifs_p05/"
#get CHIRTS function
.getCHIRTS <- function(.x, baseurl, bdir) {
if (.x$variable == "RH") {varname <- "RHum"} else {varname <- .x$variable}
this_url <- paste0(baseurl, varname, "/", .x$year, "/", .x$filename)
outfolder <- paste0(bdir, "/", varname, "/", .x$year)
if (!file.exists(outfolder)) {dir.create(outfolder, recursive=TRUE)}
setwd(outfolder)
status <- geodata:::.downloadDirect(url=this_url,
filename=.x$filename,
unzip = FALSE,
quiet = FALSE,
mode = "wb",
cacheOK = FALSE)
setwd(basedir)
return(status)
}
#kick off download
download_stat <- file_tb %>%
base::split(.[,c("filename")]) %>%
purrr::map(baseurl=base_url, bdir=basedir, .f=.getCHIRTS) %>%
as.character(unlist(.))
#append download status to table
file_tb <- file_tb %>%
dplyr::mutate(status=download_stat)
#return values of function
return(file_tb)
}
#run function
out_table <- CHIRTS_data(vars=c("RH", "Tmax", "Tmin"),
years=1983:2016,
basedir=Out)
# Download global CHIRPS
# By: H. Achicanoy
# December, 2022
# R options
g <- gc(reset = T); rm(list = ls()) # Empty garbage collector
options(warn = -1, scipen = 999)    # Remove warning alerts and scientific notation
suppressMessages(library(pacman))
suppressMessages(pacman::p_load(tidyverse,terra,lubridate,R.utils))
# Time frame
start <- 1983
end <- 2016
# Output directory
setwd("~/Desktop/BioD/")
Out <- "/Users/yacoub/Desktop/BioD/Migrations_Works/ClimateRiskPr/1.Data/Chirts/"
dir.create(Out, recursive = TRUE, showWarnings = FALSE)
#download CHIRTS data
#JRV, 2022
#load needed libraries
library(geodata)
library(tidyverse)
#function to get CHIRTS data (Tmax, Tmin, RH).
CHIRTS_data <- function(vars=c("Rh", "Tmax", "Tmin"), years=start:end, basedir=Out) {
#create table of files and file names e.g., RH.1983.01.01.tif
file_tb <- expand.grid(day=1:31, month=1:12, year=years, variable=vars)
#file_tb <- expand.grid(day=1:10, month=1, year=1983, variable=c("RH"))
filenames <- file_tb %>%
base::split(.[,c("day","month","year","variable")]) %>%
purrr::map(.f=function(.x) {
fname <- paste0(.x$variable, ".", .x$year, ".", sprintf("%02.0f", .x$month), ".", sprintf("%02.0f", .x$day), ".tif")
return(fname)
}) %>%
as.character(unlist(.))
file_tb <- file_tb %>%
dplyr::mutate(filename=filenames)
#base url
base_url <- "http://data.chc.ucsb.edu/products/CHIRTSdaily/v1.0/global_tifs_p05/"
#get CHIRTS function
.getCHIRTS <- function(.x, baseurl, bdir) {
if (.x$variable == "RH") {varname <- "RHum"} else {varname <- .x$variable}
this_url <- paste0(baseurl, varname, "/", .x$year, "/", .x$filename)
outfolder <- paste0(bdir, "/", varname, "/", .x$year)
if (!file.exists(outfolder)) {dir.create(outfolder, recursive=TRUE)}
setwd(outfolder)
status <- geodata:::.downloadDirect(url=this_url,
filename=.x$filename,
unzip = FALSE,
quiet = FALSE,
mode = "wb",
cacheOK = FALSE)
setwd(basedir)
return(status)
}
#kick off download
download_stat <- file_tb %>%
base::split(.[,c("filename")]) %>%
purrr::map(baseurl=base_url, bdir=basedir, .f=.getCHIRTS) %>%
as.character(unlist(.))
#append download status to table
file_tb <- file_tb %>%
dplyr::mutate(status=download_stat)
#return values of function
return(file_tb)
}
#run function
out_table <- CHIRTS_data(vars=c("RH", "Tmax", "Tmin"),
years=1992:2022,
basedir=Out)
